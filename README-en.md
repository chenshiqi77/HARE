<div align="center">

<img src="./assets/logo.jpg" width="230"/>

# HARE

[ä¸­æ–‡](./README.md) ï½œ English
<p align="center">
    ðŸ¤— <a href="https://huggingface.co/LiteAI-Team/Hare-1.1B-base">Hugging Face</a> | ðŸ¤– <a href="">ModelScope</a> | ðŸ“ƒ <a href="https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25">Technical Report</a> 
</p>
<!-- | ðŸ“‘ <a href="">ArXiv</a> -->
</div>

<!-- Introduction -->
## Introduction

HARE is a pre-trained model trained by the LiteAI team based on a mixture of open source high-quality pre-trained data of about 600B Tokens and strategy-generated training data. The model size is only 1.1B and has achieved good results on the Open LLM Leaderboard.
- We selected Mistral as the infrastructure, reused its word segmenter, and modified the model parameters to reduce the model size to 1.1B.
- Our model follows the Mistral infrastructure, so it can be directly applied to many open source projects that support Mistral, such as vLLM.
- The number of parameters of our model is only 1.1 billion, so we can deploy the model on low-cost devices such as consumer graphics cards and mobile phones.
- We compared the work of [Octopus](https://huggingface.co/NexaAIDev/Octopus-v2) and tried and successfully reproduced its work.
- We explored efficient training under FP8 precision and summarized a best practice, hoping to make our best contribution to the open source community LLM training.
- We are developing and adapting to Chinese.

Our source code is open sourced under Apache 2.0. As our model is only used for academic research, we cannot guarantee the accuracy of the content generated by the model. Please be aware of this before using it.


#### Quick Navigation

[Update log](#update_log) | [Model link](#model_link) | [Evaluation](#evaluation) | [Quick start](#quick_start) | [Continue train](#continue_train) | [Tool calling](#tool_calling) 

<!-- æ›´æ–°æ—¥å¿— -->
<p id="update_log"></p>

<!-- TODO -->
## Update Log
 - **2024-06-05 Open source [HARE-1.1B-base](https://huggingface.co/LiteAI-Team/Hare-1.1B-base), [HARE-1.1B-chat]() and tool calling practice [HARE-1.1B-tool](), you can read our technical report [here](https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25).**
<p id="model_link"></p>

## Model Address

Our model parameters and training details are as follows:

| Setting | Description |
|:---:|:---:|
|Size|1.1B|
|Model structure|Mistral|
|Model settings| Hidden size:2048, Hidden layers:22, KV heads:8, Attention heads:32|
|Batch size|2M|
|Training tokens| ~ 600B|
|Training sequence length|2048|
|Learning Rate|5e-4|
|Hardware| 16 H800-80G GPUs|

**You can go to HuggingFace or ModelScope to download and experience our model:**

<!-- TODO -->
|      | HuggingFace | ModelScope |
|:-----|:--------|:-------|
|Base|[HARE-1.1B-base](https://huggingface.co/LiteAI-Team/Hare-1.1B-base)|[HARE-1.1B-base]()|
|Chat|[HARE-1.1B-chat]()|[HARE-1.1B-chat]()|
|Tool demo|[HARE-1.1B-tool]()|[HARE-1.1B-tool]()|

**We will open source the Chinese version soon.**

<!-- è¯„æµ‹ç»“æžœ -->
<p id="evaluation"></p>

## Evaluation Results

HARE adopts a mixed training method of open source high-quality pre-training data and policy generation data. With limited training resources and a small number of pre-training tokens, it has achieved excellent results in the lightweight model (less than 2B parameters) of Open LLM Leaderboard.

|Model|Size|avg|MMLU|ARC-C|TruthfulQA 0-shot|Winogrande5-shot|Hellaswag 10-shot|GSM8K 5-shot|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
||||5-shot|25-shot|0-shot|5-shot|10-shot|5-shot|
|phi-1_5|1.3B|47.69|43.89|52.9|40.89|72.22|63.79|12.43|
|Qwen-1.5|1.8B|46.55|46.71|37.88|39.43|60.3|61.42|33.59| 
|stablelm-2|1.6B|45.25|38.95|43.34|36.78|64.56|70.45|17.44| 
|__Hare__|1.1B|40.17|35.74|38.4|42.08|59.27|57.46|8.04|
|H2o-danube|1.8B|39.12|25.94|39.42|33.86|64.48|69.58|1.44|
|OpenELM|1.1B|38.47|27.05|36.69|33.86|63.22|65.71|1.21|
|csg-wukong|1B|37.78|25.33|37.71|42.79|56.67|58.93|5.23|
|TinyLlama-3T|1.1B|36.42|26.04|33.87|37.32|59.51|60.31|1.44|

At the same time, we explored and experimented with the benchmark data leakage issue. For detailed analysis, please refer to our technical report [HARE](https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25).

Similarly, we also evaluated the model after SFT, and the results are as follows:

|Model|Size|avg|MMLU|ARC-C|TruthfulQA 0-shot|Winogrande5-shot|Hellaswag 10-shot|GSM8K 5-shot|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
||||5-shot|25-shot|0-shot|5-shot|10-shot|5-shot|
|__Hare__|1.1B|40.00|33.62|37.46|41.49|58.88|53.03|15.54|
|Qwen-1.5|1.8B|43.99|45.87|38.74|40.62|59.67|60.02|19.03| 
|stablelm-2|1.6B|50.71|41.47|43.52|46.50|64.72|69.24|38.32|
|TinyLlama|1.1B|36.26|26.22|33.53|36.79|60.22|59.38|1.44|
|cosmo|1.8B|36.59|26.69|38.57|38.15|55.49|55.13|5.53|


You can also view the evaluation rankings on [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) .

<!-- å¿«é€Ÿä½¿ç”¨ -->
<p id="quick_start"></p>

## Quick Use

Here are some usage examples. You can refer to these codes to quickly load and experience our model.

Before starting, please make sure that you have installed the necessary dependencies:
```Shell
pip install -r requirements.txt
```

You can also install [flash-attention](https://github.com/Dao-AILab/flash-attention) to speed up model reasoning and reduce video memory usage.


### Transformers Loading And Use

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_path = "LiteAI-Team/Hare-1.1B-base"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
model.to(device)

prompt = "Write a poem based on the landscape of Guizhou:"
tokens = tokenizer(prompt, add_special_tokens=True, return_tensors='pt').to(device)
output = model.generate(**tokens,max_new_tokens=128)

output_tokens = output[0].cpu().numpy()[tokens.input_ids.size()[1]:]
output_string = tokenizer.decode(output_tokens)
print(output_string)
>> """The Guizhou landscape is a sight to behold,
A place where nature's beauty is unmatched,
A land of towering mountains and vast plains,
A paradise for those who seek to explore.

The mountains rise high above the sky,
A sight to beholder, a sight to see,
The valleys stretch out as far as the eye can see,
A landscape of endless beauty and grace."""
```

For more details, please refer to [here](./examples/hf_demo/simple_example.py).

### vLLM Accelerates Inference

Because we use the model structure of Mistral, we can easily use vLLM to load our model and perform reasoning.

Before loading the model, please make sure you have installed vLLM:

```shell
pip install vllm
```

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

model_path = "LiteAI-Team/Hare-1.1B-base"
llm = LLM(model=model_path, tensor_parallel_size=4)

query = "Write a poem based on the landscape of Guizhou:"
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=64)
outputs = llm.generate(query, sampling_params)
print(outputs)
```
For more details, please refer to [here](./examples/vllm_demo/vllm_inference.py).

### Gradio Page Deployment
If you need to use Gradio for page deployment, you can refer to [gradio_demo.py](./examples/gradio_demo/gradio_demo.py).

### GPTQ Quantization

We have not yet provided any official quantization version. If you need to quantify our model, you can refer to the following operations:

* Quantization
```Shell
pip install auto-gptq
cd examples/autogptq_demo
python quantify.py \
    --original_model_path=LiteAI-Team/Hare-1.1B-base \
    --quantization_model_path=LiteAI-Team/Hare-1.1B-base-int8 \
    --quantization=8
```
* Loading Quantization Model & Inference
```python
import torch

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer, TextGenerationPipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
model_path = "LiteAI-Team/Hare-1.1B-base"

model = AutoGPTQForCausalLM.from_quantized(model_path, device=device)
tokenizer = AutoTokenizer.from_pretrained(model_path)

query = "Write a poem based on the landscape of Guizhou:"
# inference with model.generate
print(tokenizer.decode(model.generate(**tokenizer(query, return_tensors="pt").to(model.device))[0]))

# or you can also use pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(query)[0]["generated_text"])
```
For more details, please refer to [here](./examples/autogptq_demo).

### llama.cpp
If you need to use CPU for deployment and inference testing, we recommend that you use the [llama.cpp](https://github.com/ggerganov/llama.cpp) project.

1. clone llama.cpp and compile
```Shell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

2. convert the model stored in safetensors to gguf format
```Shell
python3 convert-hf-to-gguf.py models/mymodel/
```

3. quantized model
```Shell
./quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M
```

4. CPU load quantized model & inference
```Shell
 ./main -m ./models/mymodel/ggml-model-Q4_K_M.gguf -n 128 --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256
 ```

### Mobile Deployment

Our model has only 1.1B parameters. After Int4 quantization, the model only occupies 0.6G space and can be easily deployed on the mobile phone.

 - **Android**ï¼šWe chose [MLC-LLM](https://llm.mlc.ai/) as the deployment framework and deployed the Chat model on Redmi K40.

<table align="center">
    <p align="center">
      <img src="./assets/ori1_1.gif"/>
      <img src="./assets/ori2_2.gif"/>
    </p>
</table>

 - **iOS** & **HarmonyOS**ï¼šWe will deploy and test the above devices in the future.



<!-- äºŒæ¬¡å¼€å‘ -->
<p id="continue_train"></p>

## Secondary Development

### Continue Training

As of the release date, Hare-1.1B-base has trained about 600B tokens on data generated by [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B), [Cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia) and our own strategy. If you want to try to continue training, you can refer to [pretrain](./train/pretrain/) for continued training.

### FP8 Efficient Training

FP8 precision training is an emerging method for training LLM, which can greatly save video memory and improve training efficiency, but there is a lack of relevant guidance materials in the open source community. We have explored and studied FP8 precision efficient training and summarized the problems we encountered into a best practice. If you need, you can refer to [pretrain_fp8](./train/pretrain_fp8/) for FP8 training.

### SFT

#### Inference

Our Chat model, based on Mistral, adds Special Token and modifies the default chat template.
```Plaintext
<round_start>system
You are a helpful assistant.<round_end>
<round_start>user
Hello!<round_end>
<round_start>assistant
Hello there! What can i do for you?<round_end>
```
<!-- TODO -->
You can refer to [here](./examples/chat_demo/hf_chat_inference.py) to experience our released [HARE-1.1B-chat]().

#### Fine-tuning

We fine-tune our base model based on the [Firefly](https://github.com/yangjianxin1/Firefly) project. You can fine-tune our model according to the following process:

Step.0 **Add Special Token**

You can easily use Tokenizer.add_tokens() in transformers to add Special Token. We add <round_start>, <round_end> and <api_idx> to Tokenizer to reproduce the work of [Octopus](https://huggingface.co/NexaAIDev/Octopus-v2).

Step.1 **Register Chat Template**

You need to register the Chat template in Firefly/component/template.py of the Firefly project:
```Python
register_template(
    template_name='hare',
    system_format='<round_start>system\n{content}<round_end>\n',
    user_format='<round_start>user\n{content}<round_end>\n<round_start>assistant\n',
    assistant_format='{content}<round_end>\n',
    system="You are a helpful assistant.",
    stop_word='<round_end>'
)
```

Step.2 **Start Fine-tuning**

When you are ready to fine-tune the data, you can follow the official guidance of Firefly to fine-tune our model.


<!-- å·¥å…·è°ƒç”¨å®žè·µ -->
<p id="tool_calling"></p>

## Tool Call Practice

In order to fully utilize the advantages of small models in terminal deployment, we compared the work of [Octopus v2](https://huggingface.co/NexaAIDev/Octopus-v2) and successfully implemented the Android system API call and tool call capabilities in combined scenarios on the mobile phone.

**Show Video**

If you are interested in tool calls for small models on the terminal, you can read our [Technical Report](https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25), and you are also welcome to discuss and study with us.

## Statement

### License

* The code in this project is open source in accordance with the Apache-2.0 protocol.
* The Hare series model weights are currently only fully open to academic research.

### Statement

 * Hare is a language model trained based on a mixture of open source pre-trained data and strategy-synthesized pre-trained data. It does not have the ability to make value judgments, and cannot understand or express personal opinions. The output of the model does not represent the opinions and positions of the LiteAI development team.
 * Therefore, the content you generate using Hare may contain biased opinions and untrue situations. Please use it at your discretion.
 * Similarly, we will not bear any risks and problems caused by users intentionally using Hare to generate harmful content.

### Citation
If you think our work is helpful to you, you are welcome to cite our work!
```plaintext
```

## Contact us
If you have any comments or suggestions on our work, you are welcome to contact us (<chensq27@chinatelecom.cn>)!
