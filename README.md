<div align="center">

<img src="./assets/logo.jpg" width="230"/>

# HARE

ä¸­æ–‡ ï½œ [English](./README-en.md)
<p align="center">
    ğŸ¤— <a href="https://huggingface.co/LiteAI-Team/Hare-1.1B-base">Hugging Face</a> | ğŸ¤– <a href="https://modelscope.cn/models/LiteAITeam/Hare-1.1B-base">ModelScope</a> | ğŸ“ƒ <a href="https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25">Technical Report</a> 
</p>
<!-- | ğŸ“‘ <a href="">ArXiv</a> -->
</div>

<!-- Introduction -->
## ç®€ä»‹

HARE æ˜¯ç”±ä¸­å›½ç”µä¿¡è‚¡ä»½æœ‰é™å…¬å¸è´µå·åˆ†å…¬å¸ LiteAI å›¢é˜Ÿå¼€å‘çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çº¦600B Tokensçš„é«˜è´¨é‡å¼€æºå’Œç­–ç•¥ç”Ÿæˆçš„åˆæˆæ•°æ®ä½œä¸ºé¢„è®­ç»ƒæ•°æ®ã€‚æ¨¡å‹å¤§å°ä»…æœ‰1.1Bï¼Œå¹¶åœ¨Open LLM Leaderboardä¸Šå–å¾—ä¸é”™çš„æˆç»©ã€‚
 - æˆ‘ä»¬é€‰å– Mistral ä½œä¸ºåŸºç¡€æ¶æ„ï¼Œå¤ç”¨å…¶åˆ†è¯å™¨ï¼Œå¹¶ä¿®æ”¹æ¨¡å‹å‚æ•°ä½¿å¾—æ¨¡å‹å¤§å°ç¼©å°åˆ°1.1Bã€‚
 - æˆ‘ä»¬æ¨¡å‹éµå¾ª Mistral åŸºç¡€æ¶æ„ï¼Œå› æ­¤ï¼Œå¯ä»¥ç›´æ¥åº”ç”¨åœ¨è®¸å¤šæ”¯æŒ Mistral çš„å¼€æºé¡¹ç›®ä¸­ï¼Œå¦‚ vLLM ç­‰ã€‚
 - æˆ‘ä»¬æ¨¡å‹çš„å‚æ•°é‡ä»…ä¸º11äº¿ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹éƒ¨ç½²åˆ°æ¶ˆè´¹çº§æ˜¾å¡ã€æ‰‹æœºç«¯ç­‰æˆæœ¬è¾ƒä½çš„è®¾å¤‡ä¸Šã€‚
 - æˆ‘ä»¬å¯¹ç…§ [Octopus](https://huggingface.co/NexaAIDev/Octopus-v2) çš„å·¥ä½œï¼Œå°è¯•å¹¶æˆåŠŸå¤ç°äº†å…¶å·¥ä½œã€‚
 - æˆ‘ä»¬æ¢ç´¢äº†FP8ç²¾åº¦ä¸‹çš„é«˜æ•ˆè®­ç»ƒï¼Œå¹¶æ€»ç»“äº†ä¸€ä»½æœ€ä½³å®è·µï¼Œå¸Œæœ›èƒ½ä¸ºå¼€æºç¤¾åŒºLLMè®­ç»ƒä½œå‡ºåŠ›æ‰€èƒ½åŠçš„è´¡çŒ®ã€‚
 - æˆ‘ä»¬æ­£åœ¨ç ”å‘ä¸é€‚é…ä¸­æ–‡ã€‚

æˆ‘ä»¬çš„æºä»£ç éµå¾ª Apache 2.0 è¿›è¡Œå¼€æºã€‚å¯¹äºæˆ‘ä»¬çš„æ¨¡å‹ï¼Œç”±äºæˆ‘ä»¬ä»…ç”¨äºå­¦æœ¯æ€§è´¨çš„ç ”ç©¶ï¼Œå› æ­¤æ— æ³•ä¿è¯æ¨¡å‹ç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§ï¼Œè¯·æ‚¨åœ¨ä½¿ç”¨å‰æ‚‰çŸ¥ã€‚

#### å¿«é€Ÿå¯¼èˆª

[æ›´æ–°æ—¥å¿—](#update_log) | [æ¨¡å‹åœ°å€](#model_link) | [è¯„æµ‹ç»“æœ](#evaluation) | [å¿«é€Ÿä½¿ç”¨](#quick_start) | [äºŒæ¬¡å¼€å‘](#continue_train) | [å·¥å…·è°ƒç”¨å®è·µ](#tool_calling) | [è”ç³»æˆ‘ä»¬](#contact_us) 

<!-- æ›´æ–°æ—¥å¿— -->
<p id="update_log"></p>

<!-- TODO -->
## æ›´æ–°æ—¥å¿—
 - **2024-06-05 å¼€æº [HARE-1.1B-base](https://huggingface.co/LiteAI-Team/Hare-1.1B-base)ã€[HARE-1.1B-chat]() å’Œå·¥å…·è°ƒç”¨å®è·µ [HARE-1.1B-tool]()ï¼Œæ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25)é˜…è¯»æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šã€‚**

<!-- æ¨¡å‹åœ°å€ -->
<p id="model_link"></p>

## æ¨¡å‹åœ°å€

æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°åŠè®­ç»ƒç»†èŠ‚å¦‚ä¸‹ï¼š

| Setting | Description |
|:---:|:---:|
|Size|1.1B|
|Model structure|Mistral|
|Model settings| Hidden size:2048, Hidden layers:22, KV heads:8, Attention heads:32|
|Batch size|2M|
|Training tokens| ~ 600B|
|Training sequence length|2048|
|Learning Rate|5e-4|
|Hardware| 16 H800-80G GPUs|

**æ‚¨å¯ä»¥å‰å¾€HuggingFaceæˆ–æ˜¯ModelScopeä¸‹è½½å’Œä½“éªŒæˆ‘ä»¬çš„æ¨¡å‹ï¼š**

<!-- TODO -->
|      | HuggingFace | ModelScope |
|:-----|:--------|:-------|
|Base|[HARE-1.1B-base](https://huggingface.co/LiteAI-Team/Hare-1.1B-base)|[HARE-1.1B-base](https://modelscope.cn/models/LiteAITeam/Hare-1.1B-base)|
|Chat|[HARE-1.1B-chat]()|[HARE-1.1B-chat]()|
|Tool demo|[HARE-1.1B-tool]()|[HARE-1.1B-tool]()|

**æˆ‘ä»¬å°†åœ¨ä¸ä¹…åå¼€æºä¸­æ–‡ç‰ˆæœ¬ã€‚**

<!-- è¯„æµ‹ç»“æœ -->
<p id="evaluation"></p>

## è¯„æµ‹ç»“æœ

HARE é‡‡å–å°†å¼€æºé«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®å’Œç­–ç•¥ç”Ÿæˆæ•°æ®æ··åˆè®­ç»ƒçš„æ–¹å¼ï¼Œåœ¨æœ‰é™çš„è®­ç»ƒèµ„æºå’Œå°‘é‡é¢„è®­ç»ƒTokensä¸‹ï¼Œåœ¨Open LLM Leaderboardçš„è½»é‡çº§æ¨¡å‹ï¼ˆå‚æ•°é‡å°äº2Bï¼‰ä¸­ï¼Œå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ã€‚

|Model|Size|avg|MMLU|ARC-C|TruthfulQA 0-shot|Winogrande5-shot|Hellaswag 10-shot|GSM8K 5-shot|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
||||5-shot|25-shot|0-shot|5-shot|10-shot|5-shot|
|phi-1_5|1.3B|47.69|43.89|52.9|40.89|72.22|63.79|12.43|
|Qwen-1.5|1.8B|46.55|46.71|37.88|39.43|60.3|61.42|33.59| 
|stablelm-2|1.6B|45.25|38.95|43.34|36.78|64.56|70.45|17.44| 
|__Hare__|1.1B|40.17|35.74|38.4|42.08|59.27|57.46|8.04|
|H2o-danube|1.8B|39.12|25.94|39.42|33.86|64.48|69.58|1.44|
|OpenELM|1.1B|38.47|27.05|36.69|33.86|63.22|65.71|1.21|
|csg-wukong|1B|37.78|25.33|37.71|42.79|56.67|58.93|5.23|
|TinyLlama-3T|1.1B|36.42|26.04|33.87|37.32|59.51|60.31|1.44|

åŒæ—¶ï¼Œæˆ‘ä»¬é’ˆå¯¹ benchmark æ•°æ®æ³„æ¼é—®é¢˜åšäº†æ¢ç´¢ä¸å®éªŒï¼Œè¯¦ç»†åˆ†æè¯·å‚è€ƒæˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Š [HARE](https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25) ã€‚

åŒæ ·åœ°ï¼Œæˆ‘ä»¬ä¹Ÿå¯¹SFTåçš„æ¨¡å‹è¿›è¡Œè¯„æµ‹ï¼Œç»“æœå¦‚ä¸‹ï¼š

|Model|Size|avg|MMLU|ARC-C|TruthfulQA 0-shot|Winogrande5-shot|Hellaswag 10-shot|GSM8K 5-shot|
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
||||5-shot|25-shot|0-shot|5-shot|10-shot|5-shot|
|__Hare__|1.1B|40.00|33.62|37.46|41.49|58.88|53.03|15.54|
|Qwen-1.5|1.8B|43.99|45.87|38.74|40.62|59.67|60.02|19.03| 
|stablelm-2|1.6B|50.71|41.47|43.52|46.50|64.72|69.24|38.32|
|TinyLlama|1.1B|36.26|26.22|33.53|36.79|60.22|59.38|1.44|
|cosmo|1.8B|36.59|26.69|38.57|38.15|55.49|55.13|5.53|


æ‚¨ä¹Ÿå¯ä»¥åœ¨ [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) æŸ¥çœ‹è¯„æµ‹æ’åã€‚

<!-- å¿«é€Ÿä½¿ç”¨ -->
<p id="quick_start"></p>

## å¿«é€Ÿä½¿ç”¨

ä»¥ä¸‹æ˜¯ä¸€äº›ä½¿ç”¨ç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥å‚è€ƒè¿™äº›ä»£ç æ¥å¿«é€ŸåŠ è½½å¹¶ä½“éªŒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·æ‚¨ç¡®ä¿å·²ç»å®‰è£…å¿…è¦çš„ä¾èµ–åº“ï¼š
```Shell
pip install -r requirements.txt
```

æ‚¨ä¹Ÿå¯ä»¥å®‰è£… [flash-attention](https://github.com/Dao-AILab/flash-attention) æ¥åŠ é€Ÿæ¨¡å‹æ¨ç†å’Œé™ä½æ˜¾å­˜å ç”¨ã€‚


### Transformers åŠ è½½å’Œä½¿ç”¨

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
model_path = "LiteAI-Team/Hare-1.1B-base"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
model.to(device)

prompt = "Write a poem based on the landscape of Guizhou:"
tokens = tokenizer(prompt, add_special_tokens=True, return_tensors='pt').to(device)
output = model.generate(**tokens,max_new_tokens=128)

output_tokens = output[0].cpu().numpy()[tokens.input_ids.size()[1]:]
output_string = tokenizer.decode(output_tokens)
print(output_string)
>> """The Guizhou landscape is a sight to behold,
A place where nature's beauty is unmatched,
A land of towering mountains and vast plains,
A paradise for those who seek to explore.

The mountains rise high above the sky,
A sight to beholder, a sight to see,
The valleys stretch out as far as the eye can see,
A landscape of endless beauty and grace."""
```

### vLLM åŠ é€Ÿæ¨ç†

å› ä¸ºæˆ‘ä»¬æ²¿ç”¨äº† Mistral çš„æ¨¡å‹ç»“æ„ï¼Œå› æ­¤ï¼Œå¯ä»¥å¾ˆæ–¹ä¾¿çš„ä½¿ç”¨ vLLM æ¥åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†ã€‚

åœ¨æ¨¡å‹åŠ è½½å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»å®‰è£…å¥½vLLMï¼š

```shell
pip install vllm
```

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

model_path = "LiteAI-Team/Hare-1.1B-base"
llm = LLM(model=model_path, tensor_parallel_size=4)

query = "Write a poem based on the landscape of Guizhou:"
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=64)
outputs = llm.generate(query, sampling_params)
print(outputs)
```

### Gradio é¡µé¢éƒ¨ç½²
å¦‚æ‚¨éœ€è¦ä½¿ç”¨Gradioè¿›è¡Œé¡µé¢éƒ¨ç½²ï¼Œæ‚¨å¯å‚è€ƒ [gradio_demo.py](./examples/gradio_demo/gradio_demo.py) ã€‚

åœ¨è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿æ‚¨å®‰è£…å¥½ç›¸å…³ä¾èµ–ï¼š
```Shell
pip install fastapi
pip install uvicorn
pip install gradio
```

æ‚¨å¯ä»¥å‚è€ƒä»¥ä¸‹ä»£ç å¯åŠ¨ Web UIï¼š
```Shell
cd examples/gradio_demo
uvicorn gradio_demo:app --host 0.0.0.0 --port 4999
```

ç­‰å¾…æœåŠ¡å¯åŠ¨ï¼Œå³å¯è®¿é—® http://0.0.0.0:4999/gradio/ ä½“éªŒ Web UI


### GPTQ é‡åŒ–

æˆ‘ä»¬æš‚æœªæä¾›ä»»ä½•å®˜æ–¹é‡åŒ–ç‰ˆæœ¬ï¼Œå¦‚æ‚¨éœ€è¦é‡åŒ–ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥å‚è€ƒå¦‚ä¸‹æ“ä½œï¼š

* é‡åŒ–
```Shell
pip install auto-gptq
cd examples/autogptq_demo
python quantify.py \
    --original_model_path=LiteAI-Team/Hare-1.1B-base \
    --quantization_model_path=LiteAI-Team/Hare-1.1B-base-int8 \
    --quantization=8
```
* åŠ è½½é‡åŒ–æ¨¡å‹ & æ¨ç†
```python
import torch

from auto_gptq import AutoGPTQForCausalLM
from transformers import AutoTokenizer, TextGenerationPipeline

device = "cuda:0" if torch.cuda.is_available() else "cpu"
model_path = "LiteAI-Team/Hare-1.1B-base"

model = AutoGPTQForCausalLM.from_quantized(model_path, device=device)
tokenizer = AutoTokenizer.from_pretrained(model_path)

query = "Write a poem based on the landscape of Guizhou:"
# inference with model.generate
print(tokenizer.decode(model.generate(**tokenizer(query, return_tensors="pt").to(model.device))[0]))

# or you can also use pipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline(query)[0]["generated_text"])
```
æ›´å¤šç»†èŠ‚è¯·å‚è€ƒ[è¿™é‡Œ](./examples/autogptq_demo)ã€‚

### llama.cpp
å¦‚æ‚¨éœ€è¦ä½¿ç”¨CPUè¿›è¡Œéƒ¨ç½²å’Œæ¨ç†æµ‹è¯•ï¼Œæˆ‘ä»¬æ¨èæ‚¨ä½¿ç”¨ [llama.cpp](https://github.com/ggerganov/llama.cpp) é¡¹ç›®ã€‚

1. clone llama.cpp å¹¶ç¼–è¯‘
```Shell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
make
```

2. å°†ä»¥safetensorså­˜å‚¨çš„æ¨¡å‹è½¬æ¢ä¸ºggufæ ¼å¼
```Shell
python3 convert-hf-to-gguf.py models/mymodel/
```

3. é‡åŒ–æ¨¡å‹
```Shell
./quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M
```

4. CPUåŠ è½½é‡åŒ–æ¨¡å‹ & æ¨ç†
```Shell
 ./main -m ./models/mymodel/ggml-model-Q4_K_M.gguf -n 128 --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256
 ```

### æ‰‹æœºéƒ¨ç½²

æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°é‡ä»…æœ‰1.1Bï¼Œç»Int4é‡åŒ–åï¼Œæ¨¡å‹ä»…å ç”¨0.6Gçš„ç©ºé—´ï¼Œå¯è½»æ¾éƒ¨ç½²åœ¨æ‰‹æœºç«¯ã€‚

 - **Android**ï¼šæˆ‘ä»¬é€‰æ‹© [MLC-LLM](https://llm.mlc.ai/) ä½œä¸ºéƒ¨ç½²æ¡†æ¶ï¼Œåœ¨ Redmi K40 ä¸Šè¿›è¡Œ Chat æ¨¡å‹çš„éƒ¨ç½²æµ‹è¯•ã€‚

<table align="center">
    <p align="center">
      <img src="./assets/ori1_1.gif"/>
      <img src="./assets/ori2_2.gif"/>
    </p>
</table>

 - **iOS** & **HarmonyOS**ï¼šæˆ‘ä»¬å°†åœ¨æœªæ¥å¯¹ä¸Šè¿°è®¾å¤‡è¿›è¡Œéƒ¨ç½²æµ‹è¯•ã€‚



<!-- äºŒæ¬¡å¼€å‘ -->
<p id="continue_train"></p>

## äºŒæ¬¡å¼€å‘

### ç»§ç»­è®­ç»ƒ

æˆªè‡³å‘å¸ƒå‰ï¼ŒHare-1.1B-base åœ¨ [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B)ã€[Cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia) ä»¥åŠæˆ‘ä»¬è‡ªå·±ç­–ç•¥ç”Ÿæˆçš„æ•°æ®ä¸Šè®­ç»ƒçº¦ 600B Tokensï¼Œå¦‚æ‚¨æƒ³å°è¯•ç»§ç»­è®­ç»ƒï¼Œæ‚¨å¯ä»¥å‚è€ƒ [pretrain](./train/pretrain/) è¿›è¡Œç»§ç»­è®­ç»ƒã€‚

### FP8 é«˜æ•ˆè®­ç»ƒ

FP8ç²¾åº¦è®­ç»ƒæ˜¯ç›®å‰è®­ç»ƒ LLM çš„ä¸€ç§æ–°å…´æ–¹æ³•ï¼Œå¯ä»¥å¤§å¹…èŠ‚çœæ˜¾å­˜å¹¶æå‡è®­ç»ƒæ•ˆç‡ï¼Œä½†åœ¨å¼€æºç¤¾åŒºä¸­ç¼ºå°‘ç›¸å…³çš„æŒ‡å¯¼èµ„æ–™ã€‚æˆ‘ä»¬å¯¹FP8ç²¾åº¦é«˜æ•ˆè®­ç»ƒåšäº†æ¢ç´¢å’Œç ”ç©¶ï¼Œå°†æˆ‘ä»¬æ‰€é‡åˆ°çš„é—®é¢˜æ€»ç»“å‡ºä¸€ä»½æœ€ä½³å®è·µï¼Œå¦‚æ‚¨éœ€è¦ï¼Œæ‚¨å¯ä»¥å‚è€ƒ [pretrain_fp8](./train/pretrain_fp8/) è¿›è¡ŒFP8è®­ç»ƒï¼Œå‚è€ƒ [fp8_inference.py](./examples/fp8_inference_demo/fp8_inference.py) è¿›è¡ŒFP8æ¨ç†ã€‚

### SFT

#### æ¨ç†

æˆ‘ä»¬çš„ Chat æ¨¡å‹ï¼Œåœ¨ Mistral åŸºç¡€ä¸Šï¼Œæ–°å¢äº† Special Tokenï¼Œå¹¶ä¿®æ”¹äº†é»˜è®¤çš„ chat template
```Plaintext
<round_start>system
You are a helpful assistant.<round_end>
<round_start>user
Hello!<round_end>
<round_start>assistant
Hello there! What can i do for you?<round_end>
```
<!-- TODO -->
æ‚¨å¯ä»¥æŒ‰ç…§å‚è€ƒ[è¿™é‡Œ](./examples/chat_demo/hf_chat_inference.py)ä½“éªŒæˆ‘ä»¬å‘å¸ƒçš„ [HARE-1.1B-chat]()ã€‚

#### å¾®è°ƒ

æˆ‘ä»¬åŸºäº [Firefly](https://github.com/yangjianxin1/Firefly) é¡¹ç›®å¯¹æˆ‘ä»¬çš„ base æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æ‚¨å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æµç¨‹å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

Step.0 **æ–°å¢ Special Token**

æ‚¨å¯ä»¥å¾ˆæ–¹ä¾¿çš„ä½¿ç”¨ transformers ä¸­çš„ Tokenizer.add_tokens() æ¥æ–°å¢ Special Tokenã€‚æˆ‘ä»¬ä¸º Tokenizer æ–°å¢ <round_start> ã€ <round_end> ä»¥åŠä¸ºå¤ç° [Octopus](https://huggingface.co/NexaAIDev/Octopus-v2) å·¥ä½œçš„ <api_idx> ç­‰ Special Tokenã€‚

Step.1 **æ³¨å†ŒChatæ¨¡æ¿**

æ‚¨éœ€è¦åœ¨ Firefly é¡¹ç›®çš„ Firefly/component/template.py ä¸­ï¼Œæ³¨å†ŒChatæ¨¡æ¿ï¼š
```Python
register_template(
    template_name='hare',
    system_format='<round_start>system\n{content}<round_end>\n',
    user_format='<round_start>user\n{content}<round_end>\n<round_start>assistant\n',
    assistant_format='{content}<round_end>\n',
    system="You are a helpful assistant.",
    stop_word='<round_end>'
)
```

Step.2 **å¼€å§‹å¾®è°ƒ**

å½“æ‚¨å‡†å¤‡å¥½å¾®è°ƒæ•°æ®åï¼Œå³å¯æŒ‰ç…§ Firefly å®˜æ–¹çš„æŒ‡å¯¼å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚


<!-- å·¥å…·è°ƒç”¨å®è·µ -->
<p id="tool_calling"></p>

## å·¥å…·è°ƒç”¨å®è·µ

ä¸ºå®Œå…¨å‘æŒ¥å‡ºå°æ¨¡å‹åœ¨ç«¯ä¾§éƒ¨ç½²ä¸Šçš„ä¼˜åŠ¿ï¼Œæˆ‘ä»¬å¯¹ç…§ [Octopus v2](https://huggingface.co/NexaAIDev/Octopus-v2) çš„å·¥ä½œå¹¶ä½¿ç”¨Hare-1.1B-baseæ›¿æ¢Gemma-2Bï¼ŒæˆåŠŸåœ¨æ‰‹æœºç«¯å®ç°å®‰å“ç³»ç»ŸAPIè°ƒç”¨å’Œç»„åˆåœºæ™¯ä¸‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚

[**è§†é¢‘å±•ç¤º**](https://www.bilibili.com/video/BV1Ry411b7yx)

å¦‚æ‚¨å¯¹å°æ¨¡å‹åœ¨ç«¯ä¾§ä¸Šè¿›è¡Œå·¥å…·è°ƒç”¨æ„Ÿå…´è¶£ï¼Œæ‚¨å¯ä»¥é˜…è¯»æˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://liteai-team.notion.site/HARE-HumAn-pRiors-a-key-to-small-language-model-Efficiency-a285280a3c61491ab142cc718f84aa7d?pvs=25)ï¼Œä¹Ÿæ¬¢è¿æ‚¨ä¸æˆ‘ä»¬å…±åŒæ¢è®¨å’Œæ·±å…¥ç ”ç©¶ã€‚

## å£°æ˜

### åè®®

* æœ¬é¡¹ç›®ä¸­çš„ä»£ç ä¾ç…§ Apache-2.0 åè®®å¼€æºã€‚
* Hareç³»åˆ—æ¨¡å‹æƒé‡ç›®å‰ä»…å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ã€‚

### å£°æ˜

 * Hare æ˜¯ä¸€ä¸ªåŸºäºå¼€æºé¢„è®­ç»ƒæ•°æ®å’Œç­–ç•¥åˆæˆé¢„è®­ç»ƒæ•°æ®æ··åˆè®­ç»ƒå¾—åˆ°çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒä¸å…·å¤‡ä»·å€¼åˆ¤æ–­èƒ½åŠ›ï¼Œæ— æ³•ç†è§£ã€è¡¨è¾¾ä¸ªäººè§‚ç‚¹ï¼Œæ¨¡å‹çš„è¾“å‡ºå†…å®¹ä¸ä»£è¡¨ LiteAI å¼€å‘å›¢é˜Ÿçš„è§‚ç‚¹ä¸ç«‹åœºã€‚
 * å› æ­¤ï¼Œæ‚¨ä½¿ç”¨ Hare ç”Ÿæˆçš„å†…å®¹å¯èƒ½å­˜æœ‰åè§‚ç‚¹å’Œä¸å®æƒ…å†µï¼Œè¯·æ‚¨é…Œæƒ…ä½¿ç”¨ã€‚
 * åŒæ ·ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ç”¨æˆ·æ•…æ„ä½¿ç”¨ Hare è¿›è¡Œæœ‰å®³å†…å®¹ç”Ÿæˆæ‰€å¸¦æ¥çš„ä»»ä½•é£é™©ä¸é—®é¢˜ã€‚

### å¼•ç”¨
å¦‚æ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿æ‚¨å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼
```plaintext
```

<!-- è”ç³»æˆ‘ä»¬ -->
<p id="contact_us"></p>

## è”ç³»æˆ‘ä»¬
å¦‚æœæ‚¨å¯¹æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»»ä½•çš„æ„è§ã€å»ºè®®ï¼Œæ¬¢è¿æ‚¨ä¸æˆ‘ä»¬ï¼ˆ<chensq27@chinatelecom.cn>ï¼‰è”ç³»ï¼